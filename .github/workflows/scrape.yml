name: LTK Scraper

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      scrape_type:
        description: 'Scrape type (basic=fast listing, detail=full post data with products)'
        required: false
        default: 'detail'
        type: choice
        options:
          - basic
          - detail
      category:
        description: 'Category to scrape'
        required: false
        default: 'ltkfindsunder50'
        type: choice
        options:
          - ltkfindsunder50
          - ltkfindsunder100
          - ltksalealert
          - ltkbeauty
          - ltkhome
          - ltkfit
          - ltkworkwear
          - all
      limit:
        description: 'Max posts to scrape (detail mode only)'
        required: false
        default: '20'
        type: string

  # Scheduled runs (daily at 6 AM UTC)
  schedule:
    - cron: '0 6 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable || sudo apt-get install -y chromium-browser
          which google-chrome-stable || which chromium-browser

      - name: Run scraper
        run: |
          SCRAPE_TYPE="${{ github.event.inputs.scrape_type }}"
          CATEGORY="${{ github.event.inputs.category }}"
          LIMIT="${{ github.event.inputs.limit }}"

          # Default values for scheduled runs
          [ -z "$SCRAPE_TYPE" ] && SCRAPE_TYPE="detail"
          [ -z "$CATEGORY" ] && CATEGORY="ltkfindsunder50"
          [ -z "$LIMIT" ] && LIMIT="20"

          echo "Scrape Type: $SCRAPE_TYPE"
          echo "Category: $CATEGORY"
          echo "Limit: $LIMIT"

          if [ "$SCRAPE_TYPE" = "detail" ]; then
            echo "Running detailed scraper (captions, products, videos)..."
            npm run scrape:detail -- --category $CATEGORY --limit $LIMIT
          elif [ "$CATEGORY" = "all" ]; then
            echo "Running basic scraper on all categories..."
            npm run scrape:all
          else
            echo "Running basic scraper on $CATEGORY..."
            npm run scrape:api -- --category $CATEGORY
          fi
        env:
          PUPPETEER_EXECUTABLE_PATH: /usr/bin/google-chrome-stable

      - name: Upload scraped data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: ltk-scraped-data-${{ github.run_id }}
          path: data/scraped/*.json
          retention-days: 30

      - name: Upload to GCS (if configured)
        if: env.GCP_SA_KEY != ''
        run: npm run upload:gcs
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}

  notify:
    runs-on: ubuntu-latest
    needs: scrape
    if: failure()

    steps:
      - name: Notify on failure
        run: |
          echo "Scrape job failed! Check the logs for details."
          # Add Slack/Discord webhook notification here if needed
